#!/bin/bash

# --- 1. å‚æ•°æ¥æ”¶ (ä¸¥æ ¼ä½ç½®å‚æ•°) ---
# è„šæœ¬ 1 è°ƒç”¨ç¤ºä¾‹: bash vllm_server.sh "0" "8901" "Qwen2.5-Omni-7B"
GPUS=${1:-"0"}               # æ˜¾å¡
PORT=${2:-"8901"}            # ç«¯å£
NAME=${3:-"Qwen2.5-Omni-7B"}   # æ¨¡å‹å

# è‡ªåŠ¨ç”Ÿæˆè·¯å¾„
PATH_MOD="../models/$NAME"

# å¼ºåˆ¶é‡å¯è®¾ä¸º true (è‡ªåŠ¨åŒ–è„šæœ¬å»ºè®®å§‹ç»ˆä¸º trueï¼Œé˜²æ­¢ç«¯å£å ç”¨å¯¼è‡´å¡æ­»)
FORCE_RESTART=true

MODELS=(
    "true | $GPUS | $PORT | $NAME | $PATH_MOD"
)

# --- 2. ç«¯å£æ£€æŸ¥å‡½æ•° ---
prepare_port() {
    local port=$1
    local model_name=$2
    local pid=$(lsof -t -i:$port -sTCP:LISTEN)
    
    # åªè¦ç«¯å£è¢«å ç”¨ï¼Œå°±æ‰§è¡Œæ¸…ç†
    if [ -n "$pid" ]; then
        echo "å‘ç°ç«¯å£ $port å·²è¢« PID: $pid å ç”¨ï¼Œæ­£åœ¨æ¸…ç†..."
        kill -9 $pid 2>/dev/null
        sleep 2
        return 1 # æ¸…ç†åè¿”å›â€œå‡†å¤‡å¥½äº†â€
    fi
    return 1 # æœ¬æ¥å°±æ˜¯å¹²å‡€çš„
}

# --- 3. å¾ªç¯å¯åŠ¨é€»è¾‘ ---
for model_info in "${MODELS[@]}"; do
    IFS='|' read -r active gpus port name path <<< "$(echo $model_info | tr -d ' ')"
    
    if [ "$active" != "true" ]; then continue; fi

    # å‡†å¤‡ç«¯å£
    prepare_port $port "$name"

    # è®¡ç®— TP æ•°é‡
    tp_size=$(echo $gpus | tr -cd ',' | wc -c)
    tp_size=$((tp_size + 1))

    echo "ğŸš€ æ­£åœ¨å¯åŠ¨: $name (Port: $port, GPU: $gpus, TP: $tp_size)"
    
    # ä½¿ç”¨ if [[ ... ]] æ¨¡ç³ŠåŒ¹é…ç‰¹å¾è¯
    if [[ "$name" == *"Qwen3-Omni"* ]]; then
        CUDA_VISIBLE_DEVICES=$gpus vllm serve "$path" --port $port --host 0.0.0.0 --dtype bfloat16 --max-model-len 65536 \
            --tensor-parallel-size $tp_size --served-model-name "$name" > "${name}.log" 2>&1 &

    elif [[ "$name" == *"Voxtral"* ]]; then
        CUDA_VISIBLE_DEVICES=$gpus vllm serve "$path" --port $port --host 0.0.0.0 --dtype bfloat16 --trust-remote-code \
            --tokenizer_mode mistral --config_format mistral --load_format mistral \
            --tensor-parallel-size $tp_size --served-model-name "$name" > "${name}.log" 2>&1 &

    elif [[ "$name" == *"Phi-4"* ]]; then
        CUDA_VISIBLE_DEVICES=$gpus vllm serve "$path" --port $port --host 0.0.0.0 --dtype bfloat16 --trust-remote-code \
            --max-model-len 131072 --limit-mm-per-prompt '{"audio":3,"image":3}' \
            --enable-lora --max-loras 2 --max-lora-rank 320 \
            --lora-modules speech=../models/Phi-4-multimodal-instruct/speech-lora vision=../models/Phi-4-multimodal-instruct/vision-lora \
            --tensor-parallel-size $tp_size --served-model-name "$name" > "${name}.log" 2>&1 &

    elif [[ "$name" == *"midasheng"* ]]; then
        CUDA_VISIBLE_DEVICES=$gpus python3 -m vllm.entrypoints.openai.api_server --model "$path" \
            --port $port --host 0.0.0.0 --dtype bfloat16 --max_model_len 4096 --trust_remote_code \
            --tensor-parallel-size $tp_size --served-model-name "$name" --enable-chunked-prefill false \ > "${name}.log" 2>&1 &

    else
        # é»˜è®¤å¯åŠ¨é€»è¾‘ (Qwen2.5-Omni-7B ä¼šèµ°è¿™é‡Œ)
        CUDA_VISIBLE_DEVICES=$gpus vllm serve "$path" --port $port --host 0.0.0.0 --dtype bfloat16 --trust-remote-code \
            --tensor-parallel-size $tp_size --served-model-name "$name" > "${name}.log" 2>&1 &
    fi
done

echo "---------------------------------------"
echo "âœ… å¯åŠ¨æŒ‡ä»¤å‘é€å®Œæ¯•ã€‚ç›‘æ§æ—¥å¿—: eval/${NAME}.log"